# ğŸ¤– A Natural Language Driven Agentic AI Framework for Business Intelligence

> **An intelligent database investigation platform that transforms natural language queries into comprehensive business insights through autonomous AI agents**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2+-61DAFB.svg)](https://reactjs.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://www.postgresql.org/)
[![Google Gemini](https://img.shields.io/badge/Google-Gemini%202.5%20Flash-orange.svg)](https://ai.google.dev/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Technology Stack](#-technology-stack)
- [Tool Registry - 22 Specialized Tools](#-tool-registry---22-specialized-tools)
- [Installation & Setup](#-installation--setup)
- [Usage Guide](#-usage-guide)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [How It Works](#-how-it-works)
- [Example Use Cases](#-example-use-cases)
- [Autonomous Decision Making](#-autonomous-decision-making)
- [Comparison with Traditional Tools](#-comparison-with-traditional-tools)
- [Future Roadmap](#-future-roadmap)
- [Team](#-team)
- [License](#-license)

---

## ğŸŒŸ Overview

This project presents a revolutionary approach to database analysis by combining **Natural Language Processing (NLP)** with **Agentic AI** to create an autonomous business intelligence platform. Unlike traditional NL2SQL systems that merely convert queries to SQL, our system performs **multi-step investigations**, **autonomous reasoning**, and **intelligent analysis** without human intervention.

### What Makes This Different?

Traditional systems require:
- âŒ Manual SQL writing
- âŒ Step-by-step user guidance
- âŒ Separate tools for analysis, visualization, and anomaly detection
- âŒ Technical expertise in databases and statistics

**Our system provides:**
- âœ… Natural language input only
- âœ… Fully autonomous multi-step investigation
- âœ… Integrated analysis, visualization, and anomaly detection
- âœ… Zero technical expertise required
- âœ… Proactive insights and recommendations

---

## ğŸ¯ Key Features

### 1. **Autonomous Agentic Investigation**
- Multi-step reasoning and analysis without user intervention
- Intelligent tool selection based on query context
- Adaptive investigation workflows
- Self-correcting query refinement

### 2. **Comprehensive Tool Ecosystem (22 Tools)**
- **Database Discovery**: Schema exploration, table descriptions, sample data
- **SQL Execution & Optimization**: Query execution, validation, optimization
- **Statistical Analysis**: Column statistics, correlations, data quality
- **Anomaly Detection**: Revenue, time patterns, customer behavior anomalies
- **Visualization Generation**: Bar charts, line charts, pie charts, scatter plots
- **Business Metrics**: KPIs, business summaries, time-based analysis

### 3. **Intelligent Visualization**
- Automatic chart type selection based on data characteristics
- Dynamic Chart.js configuration generation
- Interactive, responsive visualizations
- Support for multiple chart types

### 4. **Fallback Pipeline for Reliability**
- Deterministic fallback workflow when agent encounters issues
- Guaranteed analysis delivery
- Hybrid intelligence combining system rules with LLM reasoning
- Enterprise-grade reliability

### 5. **Real-time Progress Tracking**
- Step-by-step investigation visibility
- Tool execution monitoring
- Reasoning transparency
- Interactive result exploration

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

Our system follows a clean, six-layer architecture designed for autonomous intelligence, scalability, and maintainability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND LAYER                           â”‚
â”‚  React.js UI | Progress Tracker | Chart.js | Results Panel â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API GATEWAY LAYER                        â”‚
â”‚      FastAPI REST Gateway | Endpoints | CORS Middleware    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AGENTIC INVESTIGATION ENGINE                   â”‚
â”‚   Orchestrator | Context Manager | Result Synthesizer      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DYNAMIC TOOL REGISTRY                       â”‚
â”‚              22 Specialized Analytical Tools                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TOOL CATEGORIES                          â”‚
â”‚  Database | SQL | Analysis | Visualization | Business      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EXTERNAL SERVICES                          â”‚
â”‚     Gemini LLM | PostgreSQL | Cache | Security             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Component Breakdown

#### **Layer 1: Frontend (React.js)**
- **User Interface**: Clean, intuitive chat-based interface
- **Progress Tracker**: Real-time investigation step visualization
- **Chart Renderer**: Dynamic visualization using Chart.js
- **Results Panel**: Interactive exploration of findings

#### **Layer 2: API Gateway (FastAPI)**
- **REST Endpoints**: `/investigate`, `/query`, `/tools`, `/health`
- **CORS Middleware**: Cross-origin resource sharing support
- **Request Validation**: Pydantic models for type safety
- **Error Handling**: Comprehensive error responses

#### **Layer 3: Agentic Investigation Engine**
- **Orchestrator**: Coordinates multi-step investigations
- **Context Manager**: Maintains investigation state and history
- **Result Synthesizer**: Combines results into coherent insights
- **Planning System**: Determines optimal tool execution sequence

#### **Layer 4: Dynamic Tool Registry**
- **Tool Management**: Registration, discovery, and execution
- **Schema Generation**: Automatic tool documentation for LLM
- **Execution History**: Audit trail of all tool invocations
- **Error Recovery**: Automatic retry and fallback mechanisms

#### **Layer 5: Tool Execution Layer**
Six categories with 22 specialized tools (detailed below)

#### **Layer 6: External Services**
- **Google Gemini 2.5 Flash**: NLP and reasoning
- **PostgreSQL Database**: Data storage and querying
- **Caching Layer**: Schema and result caching
- **Security**: Rate limiting and access control

---

## ğŸ› ï¸ Technology Stack

### **Backend**
| Technology | Version | Purpose |
|-----------|---------|---------|
| Python | 3.11+ | Core language |
| FastAPI | 0.104+ | Web framework |
| AsyncPG | Latest | PostgreSQL driver |
| Pydantic | Latest | Data validation |
| Google Generative AI | Latest | LLM integration |
| Uvicorn | Latest | ASGI server |

### **Frontend**
| Technology | Version | Purpose |
|-----------|---------|---------|
| React | 18.2+ | UI framework |
| Vite | 4.5+ | Build tool |
| Tailwind CSS | 3.3+ | Styling |
| Chart.js | 4.4+ | Visualization |
| React-Chartjs-2 | 5.2+ | React wrapper |
| Axios | 1.6+ | HTTP client |
| React Router | 6.20+ | Routing |

### **Database**
| Technology | Purpose |
|-----------|---------|
| PostgreSQL | Primary database |
| Neon Database | Cloud PostgreSQL hosting |

### **AI/ML**
| Technology | Purpose |
|-----------|---------|
| Google Gemini 2.5 Flash | Natural language understanding |
| Function Calling | Tool selection and parameter extraction |
| JSON Mode | Structured output generation |

---

## ğŸ”§ Tool Registry - 22 Specialized Tools

Our system includes 22 specialized tools organized into 6 categories:

### **1. Database Discovery Tools (4 Tools)**

#### `get_database_schema`
- **Purpose**: Retrieve complete database schema
- **Returns**: Tables, columns, data types, relationships
- **Use Case**: Initial database exploration

#### `describe_table`
- **Purpose**: Get detailed table metadata
- **Returns**: Column names, types, constraints, indexes
- **Use Case**: Understanding table structure

#### `get_table_sample_data`
- **Purpose**: Retrieve sample rows from a table
- **Returns**: First N rows with all columns
- **Use Case**: Understanding data format and content

#### `estimate_table_size`
- **Purpose**: Calculate table size and row count
- **Returns**: Approximate rows, disk size
- **Use Case**: Query planning and optimization

---

### **2. SQL Execution & Optimization Tools (4 Tools)**

#### `execute_sql_query`
- **Purpose**: Execute SELECT queries safely
- **Returns**: Query results as structured data
- **Use Case**: Data retrieval for analysis

#### `validate_sql_syntax`
- **Purpose**: Check SQL syntax without execution
- **Returns**: Syntax errors or validation success
- **Use Case**: Pre-execution validation

#### `explain_query_plan`
- **Purpose**: Analyze query execution plan
- **Returns**: Query plan with cost estimates
- **Use Case**: Performance optimization

#### `optimize_query`
- **Purpose**: Suggest query improvements
- **Returns**: Optimized query and recommendations
- **Use Case**: Performance tuning

---

### **3. Statistical Analysis Tools (4 Tools)**

#### `get_column_statistics`
- **Purpose**: Calculate column-level statistics
- **Returns**: Mean, median, mode, std dev, quartiles
- **Use Case**: Understanding data distribution

#### `detect_data_anomalies`
- **Purpose**: Identify statistical outliers
- **Returns**: Anomalous values with Z-scores
- **Use Case**: Data quality assessment

#### `find_correlations`
- **Purpose**: Discover relationships between columns
- **Returns**: Correlation coefficients and significance
- **Use Case**: Feature analysis

#### `analyze_data_quality`
- **Purpose**: Comprehensive data quality assessment
- **Returns**: Null counts, duplicates, value ranges
- **Use Case**: Data validation

---

### **4. Investigation & Pattern Detection Tools (3 Tools)**

#### `generate_drilldown_queries`
- **Purpose**: Create queries for deeper analysis
- **Returns**: SQL queries for dimensional breakdown
- **Use Case**: Root cause analysis

#### `compare_time_periods`
- **Purpose**: Compare metrics across time ranges
- **Returns**: Period-over-period changes
- **Use Case**: Trend analysis

#### `detect_seasonal_patterns`
- **Purpose**: Identify recurring patterns
- **Returns**: Seasonal coefficients and trends
- **Use Case**: Forecasting and planning

---

### **5. Visualization Tools (6 Tools)**

#### `generate_chart` (Smart Chart Generator)
- **Purpose**: Auto-select and generate appropriate chart
- **Returns**: Chart.js configuration
- **Use Case**: General visualization needs

#### `suggest_visualization`
- **Purpose**: Recommend chart types for data
- **Returns**: Chart type suggestions with rationale
- **Use Case**: Visualization guidance

#### `generate_bar_chart`
- **Purpose**: Create bar chart configuration
- **Returns**: Chart.js bar chart config
- **Use Case**: Category comparisons

#### `generate_line_chart`
- **Purpose**: Create line chart configuration
- **Returns**: Chart.js line chart config
- **Use Case**: Time series visualization

#### `generate_pie_chart`
- **Purpose**: Create pie chart configuration
- **Returns**: Chart.js pie chart config
- **Use Case**: Proportion display

#### `generate_scatter_plot`
- **Purpose**: Create scatter plot configuration
- **Returns**: Chart.js scatter plot config
- **Use Case**: Correlation visualization

---

### **6. Business Metrics & Anomaly Detection Tools (5 Tools)**

#### `get_key_business_metrics`
- **Purpose**: Calculate essential KPIs
- **Returns**: Revenue, orders, customers, AOV, etc.
- **Use Case**: Business performance monitoring

#### `generate_business_summary`
- **Purpose**: Comprehensive business overview
- **Returns**: Multi-dimensional business insights
- **Use Case**: Executive dashboards

#### `detect_revenue_anomalies`
- **Purpose**: Identify unusual revenue patterns
- **Returns**: Anomalous transactions with explanations
- **Use Case**: Fraud detection, quality control

#### `detect_time_pattern_anomalies`
- **Purpose**: Find irregular temporal patterns
- **Returns**: Unusual time-based behaviors
- **Use Case**: Operational monitoring

#### `detect_customer_behavior_anomalies`
- **Purpose**: Identify abnormal customer actions
- **Returns**: Unusual customer segments
- **Use Case**: Churn prediction, personalization

---

## ğŸ“¦ Installation & Setup

### Prerequisites

- **Python 3.11+**
- **Node.js 18+**
- **PostgreSQL 15+** (or Neon Database account)
- **Google Gemini API Key**

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd nl2sql
```

### Step 2: Backend Setup

```bash
# Navigate to backend directory
cd backend

# Install Python dependencies
pip install -r requirements.txt

# Create configuration file
cp env.example config.env

# Edit config.env with your credentials:
# - DATABASE_URL: Your PostgreSQL connection string
# - GEMINI_API_KEY: Your Google Gemini API key
# - CORS_ORIGINS: Frontend URLs (default: localhost:5173, localhost:3000)

# Initialize database (if needed)
cd ..
python init_database.py

# Start backend server
cd backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

The backend will be available at `http://localhost:8000`

### Step 3: Frontend Setup

```bash
# Navigate to frontend directory (in a new terminal)
cd frontend

# Install Node dependencies
npm install

# Start development server
npm run dev
```

The frontend will be available at `http://localhost:5173`

### Step 4: Verify Installation

1. **Backend Health Check**: Visit `http://localhost:8000/health`
2. **API Documentation**: Visit `http://localhost:8000/docs`
3. **Frontend**: Open `http://localhost:5173` in your browser

---

## ğŸ“– Usage Guide

### Basic Usage

1. **Open the Application**: Navigate to `http://localhost:5173`

2. **Enter a Natural Language Query**:
   ```
   "Show me the top 10 customers by revenue"
   "What were our sales trends last month?"
   "Find any unusual transactions in the last week"
   "Compare revenue between Q1 and Q2"
   ```

3. **Watch the Investigation**: The system will autonomously:
   - Understand your query intent
   - Select appropriate tools
   - Execute multi-step analysis
   - Generate visualizations
   - Provide comprehensive insights

4. **Explore Results**: 
   - View step-by-step reasoning
   - Interact with visualizations
   - Read detailed explanations
   - Ask follow-up questions

### Advanced Features

#### Custom Schema Configuration
Navigate to the Config page to provide your own database schema or connection details.

#### Investigation History
Track all past investigations and revisit previous analyses.

#### Export Results
Download visualizations and insights for reporting.

---

## ğŸ“ Project Structure

```
nl2sql/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ models.py                  # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ database.py                # Database connection manager
â”‚   â”‚   â”œâ”€â”€ gemini_client.py           # Basic Gemini LLM client
â”‚   â”‚   â”œâ”€â”€ agentic_client.py          # Agentic investigation orchestrator
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base_tool.py           # Base tool interface
â”‚   â”‚       â”œâ”€â”€ tool_registry.py       # Tool registration & management
â”‚   â”‚       â”œâ”€â”€ database_tools.py      # Database discovery tools
â”‚   â”‚       â”œâ”€â”€ sql_execution_tools.py # SQL execution tools
â”‚   â”‚       â”œâ”€â”€ analysis_tools.py      # Statistical analysis tools
â”‚   â”‚       â”œâ”€â”€ investigation_tools.py # Pattern detection tools
â”‚   â”‚       â”œâ”€â”€ visualization_tools.py # Chart generation tools
â”‚   â”‚       â”œâ”€â”€ graph_tools.py         # Specific chart types
â”‚   â”‚       â”œâ”€â”€ business_metrics_tools.py # KPI calculation tools
â”‚   â”‚       â””â”€â”€ anomaly_detection_tools.py # Anomaly detection tools
â”‚   â”œâ”€â”€ config.env                     # Environment configuration
â”‚   â”œâ”€â”€ env.example                    # Configuration template
â”‚   â””â”€â”€ requirements.txt               # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx                   # React entry point
â”‚   â”‚   â”œâ”€â”€ App.jsx                    # Main application component
â”‚   â”‚   â”œâ”€â”€ index.css                  # Global styles
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ AgenticChatPage.jsx    # Main investigation interface
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPage.jsx           # Simple NL2SQL interface
â”‚   â”‚   â”‚   â””â”€â”€ ConfigPage.jsx         # Configuration interface
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ ChartRenderer.jsx      # Chart.js wrapper component
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js                 # API client
â”‚   â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”‚   â””â”€â”€ ConfigContext.jsx      # Global configuration state
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ formatters.js          # Data formatting utilities
â”‚   â”œâ”€â”€ index.html                     # HTML template
â”‚   â”œâ”€â”€ package.json                   # Node dependencies
â”‚   â”œâ”€â”€ vite.config.js                 # Vite configuration
â”‚   â”œâ”€â”€ tailwind.config.js             # Tailwind CSS configuration
â”‚   â””â”€â”€ postcss.config.js              # PostCSS configuration
â”‚
â”œâ”€â”€ BTP_Report_Files/                  # Academic report LaTeX files
â”‚   â”œâ”€â”€ BTP_Report.tex                 # Main report file
â”‚   â”œâ”€â”€ chapter1_introduction.tex
â”‚   â”œâ”€â”€ chapter2_literature_review.tex
â”‚   â”œâ”€â”€ chapter3_methodology.tex
â”‚   â”œâ”€â”€ chapter4_implementation.tex
â”‚   â”œâ”€â”€ chapter5_results.tex
â”‚   â”œâ”€â”€ chapter6_conclusion.tex
â”‚   â”œâ”€â”€ btp_references.bib             # Bibliography
â”‚   â””â”€â”€ college_logo.jpg               # Institution logo
â”‚
â”œâ”€â”€ database_init.sql                  # Database initialization script
â”œâ”€â”€ init_database.py                   # Database setup script
â”œâ”€â”€ README.md                          # Original README
â”œâ”€â”€ PROJECT_README.md                  # This comprehensive README
â””â”€â”€ ANOMALY_DETECTION_GUIDE.md        # Anomaly detection documentation
```

---

## ğŸ”Œ API Documentation

### Core Endpoints

#### `POST /investigate`
**Autonomous agentic investigation**

**Request:**
```json
{
  "query": "What are the sales trends for the last 6 months?",
  "schema_text": "Optional: Custom schema description"
}
```

**Response:**
```json
{
  "query": "What are the sales trends for the last 6 months?",
  "investigation_steps": [
    {
      "step_number": 1,
      "reasoning": "First, I need to understand the database schema...",
      "tool_name": "get_database_schema",
      "tool_input": {},
      "result": { "tables": [...] },
      "success": true
    },
    ...
  ],
  "final_answer": "Based on the analysis, sales trends show...",
  "execution_time": 2.34
}
```

#### `POST /query`
**Simple NL2SQL execution**

**Request:**
```json
{
  "schema_text": "Table: orders (id, date, amount)",
  "query": "Show total sales"
}
```

**Response:**
```json
{
  "query": "Show total sales",
  "sql_query": "SELECT SUM(amount) as total_sales FROM orders",
  "results": [[{"total_sales": 125000}]],
  "explanation": "Query retrieves sum of all order amounts",
  "execution_time": 0.45
}
```

#### `GET /tools`
**List available tools**

**Response:**
```json
{
  "tools": [
    {
      "name": "get_database_schema",
      "description": "Retrieve database schema",
      "category": "database_discovery",
      "parameters": {}
    },
    ...
  ],
  "total_count": 22
}
```

#### `GET /health`
**System health check**

**Response:**
```json
{
  "status": "healthy",
  "database": "connected",
  "llm": "available",
  "tools_loaded": 22
}
```

---

## âš™ï¸ How It Works

### Investigation Flow

```
1. USER QUERY
   â†“
   "What were the sales trends last quarter?"
   
2. QUERY ANALYSIS
   â†“
   - Parse intent: Time series analysis
   - Identify entities: Sales, quarterly data
   - Determine required tools
   
3. SCHEMA DISCOVERY
   â†“
   - Execute: get_database_schema()
   - Identify relevant tables: orders, products
   - Understand column relationships
   
4. DATA RETRIEVAL
   â†“
   - Execute: execute_sql_query()
   - Generate optimized SQL
   - Retrieve quarterly sales data
   
5. STATISTICAL ANALYSIS
   â†“
   - Execute: get_column_statistics()
   - Calculate growth rates
   - Identify trends
   
6. ANOMALY DETECTION
   â†“
   - Execute: detect_revenue_anomalies()
   - Flag unusual patterns
   - Provide context
   
7. VISUALIZATION
   â†“
   - Execute: generate_line_chart()
   - Create Chart.js config
   - Render interactive chart
   
8. SYNTHESIS
   â†“
   - Combine all findings
   - Generate natural language summary
   - Provide actionable insights
   
9. RESPONSE DELIVERY
   â†“
   - Display step-by-step reasoning
   - Show visualizations
   - Present final answer
```

### Autonomous Decision Making

The system makes intelligent decisions at multiple levels:

#### **Tool Selection**
- **Context Analysis**: Understanding what information is needed
- **Dependency Resolution**: Determining tool execution order
- **Optimization**: Minimizing redundant operations

#### **Query Planning**
- **Schema Awareness**: Using database structure knowledge
- **Performance Optimization**: Selecting efficient queries
- **Error Recovery**: Handling failed operations gracefully

#### **Result Synthesis**
- **Multi-source Integration**: Combining data from multiple tools
- **Pattern Recognition**: Identifying insights across datasets
- **Natural Language Generation**: Creating human-readable summaries

---

## ğŸ’¡ Example Use Cases

### 1. Revenue Analysis
**Query**: "Analyze our revenue performance this month"

**System Actions**:
1. Retrieves database schema
2. Identifies orders and revenue columns
3. Executes SQL to get monthly revenue data
4. Calculates statistics (mean, median, growth)
5. Detects revenue anomalies
6. Generates trend visualization
7. Provides business summary with KPIs

**Output**: 
- Line chart showing daily revenue trends
- Statistical summary
- Anomaly highlights
- Month-over-month comparison

---

### 2. Customer Behavior Analysis
**Query**: "Find unusual customer purchasing patterns"

**System Actions**:
1. Discovers customer and order tables
2. Retrieves sample customer data
3. Calculates purchase frequency statistics
4. Runs anomaly detection on customer behavior
5. Identifies outlier customers
6. Generates scatter plot of purchase patterns
7. Provides actionable insights

**Output**:
- Scatter plot of purchase frequency vs. amount
- List of anomalous customers
- Behavioral patterns identified
- Recommendations for follow-up

---

### 3. Time Series Comparison
**Query**: "Compare Q1 2024 sales with Q1 2023"

**System Actions**:
1. Validates date columns exist
2. Retrieves quarterly data for both periods
3. Calculates period-over-period metrics
4. Detects seasonal patterns
5. Generates comparative bar chart
6. Highlights significant changes
7. Provides trend analysis

**Output**:
- Side-by-side bar chart
- Percentage changes
- Key drivers of change
- Future projections

---

### 4. Data Quality Assessment
**Query**: "Check the quality of our customer data"

**System Actions**:
1. Examines customer table structure
2. Analyzes data completeness (null values)
3. Detects duplicates
4. Validates data types and ranges
5. Identifies anomalous entries
6. Generates quality report
7. Suggests cleanup actions

**Output**:
- Data quality scorecard
- Specific issues identified
- Visualization of missing data
- Remediation recommendations

---

## ğŸ¤– Autonomous Decision Making

### What Makes Our System "Agentic"?

Traditional NL2SQL systems are **reactive** - they only do what you explicitly ask. Our system is **proactive** - it makes intelligent decisions on its own.

#### System Contributions Beyond LLM

| Capability | System Role | LLM Role |
|-----------|-------------|----------|
| **Tool Orchestration** | Determines execution order, manages dependencies | Suggests tools based on query |
| **Fallback Logic** | Automatic deterministic pipeline on failure | Not involved |
| **Schema Caching** | Intelligent caching and retrieval | Uses cached schema |
| **Query Optimization** | Rewrites queries for performance | Generates initial SQL |
| **Anomaly Detection** | Statistical algorithms (Z-score, IQR) | Interprets results |
| **Visualization Selection** | Data type analysis, chart rules | High-level suggestions |
| **Error Recovery** | Retry logic, alternative approaches | Error explanation |
| **Progress Tracking** | Real-time step updates | Not involved |

#### Example: Autonomous Tool Calling

**User Query**: "Show me sales data"

**What Happens Behind the Scenes**:

1. **LLM Decides**: Call `get_database_schema`
2. **System Automatically Adds** (without LLM asking):
   - `estimate_table_size` (to check data volume)
   - `get_table_sample_data` (to understand data format)
   
3. **LLM Decides**: Call `execute_sql_query`
4. **System Automatically Adds**:
   - `validate_sql_syntax` (pre-execution check)
   - `explain_query_plan` (performance verification)
   
5. **LLM Decides**: Generate chart
6. **System Automatically Adds**:
   - `get_column_statistics` (for appropriate scaling)
   - `detect_data_anomalies` (to highlight outliers)

**Result**: The system performed 8 operations when the LLM only requested 3!

---

## ğŸ†š Comparison with Traditional Tools

### Why Not Just Use ChatGPT, Claude, or Grok?

| Feature | Our System | ChatGPT/Claude/Grok | Traditional BI Tools |
|---------|-----------|---------------------|----------------------|
| **Natural Language Interface** | âœ… | âœ… | âŒ |
| **Autonomous Investigation** | âœ… Multi-step | âš ï¸ Single step | âŒ Manual |
| **Integrated Visualization** | âœ… Automatic | âŒ Manual | âœ… Manual |
| **Anomaly Detection** | âœ… Built-in | âŒ None | âš ï¸ Limited |
| **Database Integration** | âœ… Direct | âš ï¸ Requires setup | âœ… Direct |
| **Statistical Analysis** | âœ… Comprehensive | âŒ Basic | âœ… Comprehensive |
| **Fallback Reliability** | âœ… Guaranteed | âŒ None | âœ… Reliable |
| **Tool Ecosystem** | âœ… 22 specialized | âŒ Generic | âš ï¸ Limited |
| **Learning Curve** | âœ… Zero | âœ… Zero | âŒ High |
| **Custom Analytics** | âœ… Extensible | âŒ Fixed | âš ï¸ Complex |

### Capabilities IMPOSSIBLE with General AI Chatbots

#### 1. **Multi-Step Autonomous Investigation**
- **Chatbot**: Requires you to ask each step individually
- **Our System**: Automatically performs 5-10 related analyses

#### 2. **Integrated Statistical Analysis**
- **Chatbot**: Can't run statistical algorithms
- **Our System**: Built-in Z-score, correlation, trend detection

#### 3. **Direct Database Access**
- **Chatbot**: No database connection
- **Our System**: Real-time query execution

#### 4. **Anomaly Detection Algorithms**
- **Chatbot**: No statistical anomaly detection
- **Our System**: Revenue, time, behavior anomalies

#### 5. **Guaranteed Reliability**
- **Chatbot**: May fail without alternatives
- **Our System**: Fallback pipeline ensures results

#### 6. **Domain-Specific Tools**
- **Chatbot**: Generic capabilities only
- **Our System**: 22 business intelligence tools

#### 7. **Interactive Visualizations**
- **Chatbot**: Static images at best
- **Our System**: Chart.js interactive charts

#### 8. **Context Persistence**
- **Chatbot**: Limited conversation memory
- **Our System**: Full investigation history and context

### Real-World Scenario

**Task**: "Analyze sales and find problems"

**ChatGPT/Claude Approach**:
1. You ask: "What's in my database?"
2. ChatGPT: "I can't access your database"
3. You paste schema manually
4. You ask: "Write SQL for total sales"
5. ChatGPT provides SQL
6. You copy, run in database, paste results back
7. You ask: "Find anomalies"
8. ChatGPT: "Please provide statistical context"
9. **Result**: 15+ manual steps, no visualization, no automation

**Our System Approach**:
1. You ask: "Analyze sales and find problems"
2. **System automatically**:
   - Discovers schema
   - Queries sales data
   - Calculates statistics
   - Detects anomalies
   - Generates visualizations
   - Provides insights
3. **Result**: Complete analysis in 3 seconds, zero manual steps

---

## ğŸš€ Future Roadmap

### Phase 1: Enhanced Visualization (Dec 2024)
- [ ] Advanced chart types (heatmaps, sankey diagrams)
- [ ] Interactive dashboards
- [ ] Custom color schemes and themes
- [ ] Export to PDF/PowerPoint

### Phase 2: Performance Optimization (Jan 2025)
- [ ] Query result caching
- [ ] Parallel tool execution
- [ ] Database connection pooling
- [ ] Response time < 2 seconds target

### Phase 3: Multi-Database Support (Feb 2025)
- [ ] MySQL support
- [ ] MongoDB support
- [ ] SQLite support
- [ ] Unified query interface

### Phase 4: Advanced Analytics (Feb 2025)
- [ ] Predictive analytics
- [ ] Machine learning integrations
- [ ] What-if scenario analysis
- [ ] Automated report generation

### Phase 5: Vector Database Integration (Mar 2025)
- [ ] ChromaDB or Pinecone integration
- [ ] Conversational memory using embeddings
- [ ] Semantic context retrieval
- [ ] Cross-session learning
- [ ] Top-k relevant context injection

### Phase 6: Enterprise Features (Mar 2025)
- [ ] User authentication and authorization
- [ ] Multi-tenant support
- [ ] Audit logging
- [ ] Role-based access control

### Testing Phase (Feb - Mar 2025)
- [ ] Comprehensive unit testing
- [ ] Integration testing
- [ ] Load testing (100+ concurrent users)
- [ ] Security auditing

---

## ğŸ‘¥ Team

**Bachelor of Technology in Computer Science and Engineering**  
**Indian Institute of Information Technology Kottayam**

### Project Members

- **Arjun Deshmukh** (2022BCS0084)
- **Rishi Pramod** (2022BCS0089)
- **Harsh Jain** (2022BCS0231)
- **Lakshmi Sreya** (2022BCD0031)

### Supervisor

- **Dr. Sara Renjit**  
  Department of Computer Science and Engineering  
  IIIT Kottayam

---

## ğŸ“„ License

This project is developed as part of academic requirements for the Bachelor of Technology degree. All rights reserved by the project team and IIIT Kottayam.

---

## ğŸ™ Acknowledgments

- **Google Gemini Team** for providing the Gemini 2.5 Flash API
- **FastAPI Community** for excellent documentation
- **React.js Team** for the powerful UI framework
- **Chart.js Team** for visualization capabilities
- **IIIT Kottayam** for academic support and resources

---

## ğŸ“ Contact & Support

For questions, suggestions, or collaboration:

- **Email**: [Contact through IIIT Kottayam]
- **Institution**: Indian Institute of Information Technology Kottayam
- **Location**: Kottayam-686635, Kerala, India

---

## ğŸŒŸ Star This Project

If you find this project useful, please consider giving it a star! â­

---

**Built with â¤ï¸ by Team Agentic AI @ IIIT Kottayam**

*Last Updated: November 2025*


